{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Agenda based Conference Optimization\n",
    "---\n",
    "\n",
    "\n",
    "##  Objective\n",
    "\n",
    "The objective of this project is to:\n",
    "- Analyze research abstract titles submitted to the AIOS conference for a selected year.\n",
    "- Identify and separate unique talks from similar/duplicate ones using ML and clustering techniques.\n",
    "- Help organizers detect redundant abstracts by grouping similar titles using similarity scores.\n",
    "\n",
    "---\n",
    "\n",
    "##  Techniques & Algorithms Used\n",
    "\n",
    "- **YAKE (Yet Another Keyword Extractor):** Extracts important keywords from abstract titles to capture meaningful content.\n",
    "- **TF-IDF (Term Frequency–Inverse Document Frequency):** Transforms text data into numerical feature vectors for similarity computation.\n",
    "- **Cosine Similarity Matrix:** Measures similarity between abstract titles based on vector angles.\n",
    "- **KMeans Clustering (K=4):** Groups titles into clusters using elbow method logic for identifying similar patterns.\n",
    "- **Thresholding:** Filters pairs with cosine similarity ≥ 0.85 to detect high similarity.\n",
    "\n",
    "---\n",
    "\n",
    "##  Steps Involved: \n",
    "\n",
    "1. Load & Preprocess Data:\n",
    "\n",
    "- Read the input CSV file.\n",
    "- Extract the year from the dataset.\n",
    "- Clean and normalize the title field by:\n",
    "- Drop rows with missing required fields in conf_name and title.\n",
    "\n",
    "\n",
    "2. Filter by Year:\n",
    "\n",
    "- Select only those rows (abstracts/titles) from a specific year for analysis.\n",
    "\n",
    "3. TF-IDF Vectorization: (Each title is converted into a TF-IDF vector)\n",
    "\n",
    "- Algorithm Used: TF-IDF (Term Frequency-Inverse Document Frequency).\n",
    "\n",
    "- Input: Preprocessed titles from the filtered year.\n",
    "\n",
    "- Output: Sparse matrix of numerical features representing the text content.\n",
    "\n",
    "\n",
    "\n",
    "4. Cosine Similarity Calculation:\n",
    "\n",
    "- Algorithm Used: Cosine Similarity Matrix.\n",
    "\n",
    "- Calculate pairwise cosine similarity scores between all title vectors.\n",
    "\n",
    "- Threshold Score: Similarity > 0.85 is considered similar or duplicate.\n",
    "\n",
    "- Output: Matrix of similarity scores between every pair of titles.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "5. Cluster Similar Titles:\n",
    "\n",
    "- Algorithm Used: KMeans Clustering.\n",
    "\n",
    "- Technique: Elbow Method to determine optimal number of clusters.\n",
    "\n",
    "- Input: Titles with similarity above threshold.\n",
    "\n",
    "- Output: Clustered groups of talks that are similar or duplicates.\n",
    "\n",
    "\n",
    "\n",
    "6. Identify Unique Talks:\n",
    "- Extract talks that:\n",
    "\n",
    "  - Are not part of any pair with similarity above 0.85.\n",
    "  - Do not belong to any similarity cluster.\n",
    "\n",
    "- Output: Talks considered unique.\n",
    "\n",
    "7. Save Output CSVs:\n",
    "- CSV 1: List of unique talks (no similar title found).\n",
    "\n",
    "- CSV 2: List of similar/duplicate talks based on Similarity Score and Cluster ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dk/l893wpz54q59fjgpkm470lpw0000gn/T/ipykernel_4204/2004480184.py:24: DtypeWarning: Columns (23,24,25,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing Abstracts for Year: 2014\n",
      "\n",
      " Done!\n",
      " Similar talks saved to: similar_abstracts_2014.csv\n",
      " Unique talks saved to: unique_abstracts_2014.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import yake  # YAKE for keyword extraction (keyword ~ core meaning)\n",
    "\n",
    "# -----------------------------\n",
    "# Clean text: normalize and remove punctuation\n",
    "# -----------------------------\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower().strip()\n",
    "    text = re.sub(r\"[.,\\-;_!::‘:’:`:`]+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # normalize multiple spaces to one\n",
    "    return text\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 1: Read the input ophthalmology abstracts dataset\n",
    "# -----------------------------\n",
    "file_path = \"/Users/pranavs/Desktop/Conference Optimization/AIOC 2014 - 2025 All Abstracts Data - Shared to VS - Sheet1.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 2: Clean the dataset – Drop rows with missing fields in conf_name and title\n",
    "# -----------------------------\n",
    "df = df.dropna(subset=['# conf_name', 'title'])\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 3: Extract year from conference name (example: AIOS 2014 ➝ 2014)\n",
    "# -----------------------------\n",
    "df['year'] = df['# conf_name'].str.extract(r'(\\d{4})')\n",
    "df = df[df['year'].notnull()]\n",
    "df['year'] = df['year'].astype(int)\n",
    "\n",
    "# Clean and lowercase titles\n",
    "df['title_clean'] = df['title'].astype(str).str.lower().str.strip()\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 4: Filter by year – Select only abstracts from a specific year\n",
    "# -----------------------------\n",
    "year_to_analyze = 2014\n",
    "df_year = df[df['year'] == year_to_analyze].reset_index(drop=True)\n",
    "\n",
    "if len(df_year) < 2:\n",
    "    print(f\"Not enough entries in year {year_to_analyze}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n Processing Abstracts for Year: {year_to_analyze}\")\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 5: Extract keywords using YAKE (Yet Another Keyword Extractor)\n",
    "# -----------------------------\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"en\", n=1, top=5)\n",
    "df_year['keywords'] = df_year['title_clean'].apply(lambda text: \" \".join([kw for kw, _ in kw_extractor.extract_keywords(text)]))\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 6: TF-IDF (Term Frequency-Inverse Document Frequency) – Transform keywords into numerical feature vectors\n",
    "# -----------------------------\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(df_year['keywords'])\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 7: Cosine Similarity Matrix Approach– Compute similarity between title vectors\n",
    "# -----------------------------\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 8: Apply Thresholding  – Similarity Score > 0.85 = similar or duplicate\n",
    "#Duplicate Talks: score=1\n",
    "#Similiar Talks: score=(0.85,1)\n",
    "# -----------------------------\n",
    "threshold = 0.85\n",
    "similar_pairs = []\n",
    "used_abs_nos = set()\n",
    "\n",
    "for i in range(len(df_year)):\n",
    "    for j in range(i + 1, len(df_year)):\n",
    "        score = similarity_matrix[i, j]\n",
    "        if score >= threshold:\n",
    "            row_i = df_year.iloc[i]\n",
    "            row_j = df_year.iloc[j]\n",
    "            similar_pairs.append({\n",
    "                'year': year_to_analyze,\n",
    "                'similarity_score': round(score, 3),\n",
    "                'abs_no_1': row_i['abs_no'],\n",
    "                'name_1': row_i['name'],\n",
    "                'title_1': row_i['title'],\n",
    "                'abs_no_2': row_j['abs_no'],\n",
    "                'name_2': row_j['name'],\n",
    "                'title_2': row_j['title'],\n",
    "            })\n",
    "            used_abs_nos.update([row_i['abs_no'], row_j['abs_no']])\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 9: Identify Unique Talks – Not part of any similarity match\n",
    "# -----------------------------\n",
    "unique_df = df_year[~df_year['abs_no'].isin(used_abs_nos)][['abs_no', 'name', 'title', 'year']]\n",
    "unique_df = unique_df.sort_values(by='abs_no')\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 10: Cluster similar/duplicate talks using KMeans Clustering Algorithm (Unsupervised ML)\n",
    "# -----------------------------\n",
    "k = 4  # Can be determined via elbow method \n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "df_year['cluster'] = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "# Assign cluster info to similar pairs for context\n",
    "for pair in similar_pairs:\n",
    "    idx1 = df_year[df_year['abs_no'] == pair['abs_no_1']].index[0]\n",
    "    pair['cluster'] = df_year.loc[idx1, 'cluster']\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 11: Output Results – Save unique and similar talks to separate CSVs\n",
    "# Save to CSV ( 2 CSV's: one with unique talks and other with similar/dupliacte talks )\n",
    "# -----------------------------\n",
    "similar_df = pd.DataFrame(similar_pairs).sort_values(by=\"similarity_score\", ascending=True) #Ascending order of Similarity Score \n",
    "similar_output_path = f\"similar_abstracts_{year_to_analyze}.csv\"\n",
    "unique_output_path = f\"unique_abstracts_{year_to_analyze}.csv\"\n",
    "\n",
    "similar_df.to_csv(similar_output_path, index=False)\n",
    "unique_df.to_csv(unique_output_path, index=False)\n",
    "\n",
    "print(f\"\\n Done!\")\n",
    "print(f\" Similar talks saved to: {similar_output_path}\")\n",
    "print(f\" Unique talks saved to: {unique_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Ranking similiar/duplicate talks based on how often they appear in pairs of similar/duplicate talks\n",
    "\n",
    "To say:\n",
    "- “Which abstracts are most commonly reused or appear in multiple similar pairs?”\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Full ranked duplicate talks saved to: /Users/pranavs/Desktop/Conference Optimization/results/ranked_duplicate_talks_2014.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the similar/duplicate talks CSV\n",
    "similar_df = pd.read_csv(\"/Users/pranavs/Desktop/Conference Optimization/results/similar_abstracts_2014.csv\")\n",
    "\n",
    "# Count how often each abstract appears in similar pairs (Example: 280 -> appeared in 280 similarity pairs)\n",
    "talk_counts = pd.concat([similar_df['abs_no_1'], similar_df['abs_no_2']])\n",
    "talk_freq = talk_counts.value_counts().reset_index()\n",
    "talk_freq.columns = ['abs_no', 'similarity_count']\n",
    "\n",
    "# Merge with original titles for context\n",
    "titles = pd.concat([\n",
    "    similar_df[['abs_no_1', 'title_1']].rename(columns={'abs_no_1': 'abs_no', 'title_1': 'title'}),\n",
    "    similar_df[['abs_no_2', 'title_2']].rename(columns={'abs_no_2': 'abs_no', 'title_2': 'title'})\n",
    "]).drop_duplicates(subset='abs_no')\n",
    "\n",
    "ranked_talks = talk_freq.merge(titles, on='abs_no', how='left')\n",
    "ranked_talks = ranked_talks.sort_values(by='similarity_count', ascending=False)\n",
    "\n",
    "\n",
    "# Save the full ranked list to an output CSV file\n",
    "output_path = \"/Users/pranavs/Desktop/Conference Optimization/results/ranked_duplicate_talks_2014.csv\"\n",
    "ranked_talks.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n Full ranked duplicate talks saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
